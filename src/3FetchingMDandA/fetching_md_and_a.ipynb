{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15ccdfc0",
   "metadata": {},
   "source": [
    "\n",
    "# 4.3.3 htmファイルからMD＆A情報としてみなされる文章のみを抽出 \n",
    "\n",
    "　htmファイルから、 BeautifulSoupを用いて MD＆A情報としてみなされる文章のみを抽出した。BeautifulSoupとは、Pythonのライブラリのひとつで、htmlやxmlといったWebページで用いられるデータを構文解析する際に用いられるライブラリである。今回は、 htm形式のテキストファイルからMD&A部分を抜き出し、htm形式のタグの除去等の処理を行って生の文章を抽出した。また文章を抽出においては、加藤・五島(2020)を参考に「１【経営方針、経営環境及び対処すべき課題等】」と「３【経営者による財政状態、経営成績及びキャッシュ・フローの状況の分析】」の 2 つの章のテキストをMD&Aの記述とみなして抽出を行った。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f83123e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "import glob\n",
    "import pandas as pd\n",
    "import bs4\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "\n",
    "def fetch_md_and_a(data_frame_name, data_frame):\n",
    "\n",
    "    htm_files = call_htm_files(data_frame_name)\n",
    "    industry_htm_files_hash = make_industry_htm_files_hash(htm_files , data_frame_name, data_frame)\n",
    "    \n",
    "    sum_csv_output = 1\n",
    "    for industry_name, htm_files in industry_htm_files_hash.items():\n",
    "        \n",
    "        edinet_id_files_hash = {}\n",
    "        #Edninetの識別子ごとでファイル群を管理する\n",
    "        for file in htm_files:\n",
    "            pattern = 'E.*?-'\n",
    "            result = re.search(pattern, file)\n",
    "            start, end = result.span()\n",
    "            identifier = file[start: end-1]\n",
    "\n",
    "            if identifier in edinet_id_files_hash :\n",
    "                edinet_id_files_hash[identifier].append(file)\n",
    "            else:\n",
    "                edinet_id_files_hash[identifier] = [file]\n",
    "        \n",
    "        idx_num = 1\n",
    "        # すでにEdninetの識別子順でソートされている\n",
    "        for identifier, e_files in edinet_id_files_hash.items() :\n",
    "            # 「１【経営方針、経営環境及び対処すべき課題等】」,「３【経営者による財政状態、経営成績及びキャッシュ・フローの状況の分析】」            \n",
    "            #コロナ前, コロナ過渡期, コロナ後の最低３つがなかった時\n",
    "#             if len(e_files) < 3:\n",
    "#                 print(f\"detected: {e_files}\")\n",
    "#                 continue\n",
    "            \n",
    "            date_files_hash = {}\n",
    "            for e_f in e_files:\n",
    "                date_str = get_date_str_val(e_f)\n",
    "                if date_str in date_files_hash :\n",
    "                    date_files_hash[date_str].append(e_f)\n",
    "                else:\n",
    "                    date_files_hash[date_str] = [e_f]\n",
    "            \n",
    "            for date_str, d_files in date_files_hash.items():\n",
    "                \n",
    "                d_files_len = len(d_files)\n",
    "\n",
    "                if d_files_len > 1:\n",
    "                    print(f\"count: {d_files_len}, {d_files}\")\n",
    "                \n",
    "                md_a_df = pd.DataFrame()\n",
    "                idx = 0\n",
    "                while  idx < d_files_len :\n",
    "                    sp = read_file(d_files[idx])\n",
    "                    htm_contents = sp.find('body').div.contents\n",
    "                    md_a_contents, func =  extract_md_a_contents(htm_contents)\n",
    "                    tmp_df = func(md_a_contents)                \n",
    "                    md_a_df  = pd.concat([md_a_df, tmp_df], axis=0)\n",
    "                    idx += 1\n",
    "\n",
    "                save_md_a_df_to_csv(md_a_df, idx_num, date_str , data_frame_name,  industry_name)\n",
    "                #beforeとafterでユニークなindex_numを持つようにする\n",
    "                idx_num += 1\n",
    "                \n",
    "                print(sum_csv_output, len(md_a_df))\n",
    "                sum_csv_output += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18f2d833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_htm_files(data_frame_name):\n",
    "    htm_files = glob.glob(f\"/home/jovyan/2UnzippingHtm/UnzipedHtmFiles/{data_frame_name}/**/*.htm\", recursive=True)\n",
    "    return  htm_files\n",
    "\n",
    "\n",
    "def make_industry_htm_files_hash(htm_files , data_frame_name: str, data_frame):\n",
    "    industry_htm_files_hash = {}\n",
    "    \n",
    "    industry_list = make_type_of_industry_list(data_frame)\n",
    "    for industry in industry_list:\n",
    "        industry_htm_files_hash[industry]= list(filter(lambda x:  industry in x , htm_files))\n",
    "    \n",
    "    return  industry_htm_files_hash \n",
    "\n",
    "\n",
    "def make_type_of_industry_list(data_frame : pd.DataFrame, industry_col=\"[業種（東証）]\"):\n",
    "    return [\"サービス業\"]\n",
    "\n",
    "\n",
    "def get_date_str_val(htm_file) :\n",
    "    date_str = htm_file[-20:-10]\n",
    "    return  date_str\n",
    "\n",
    "\n",
    "def read_file(htm_path: str) -> BeautifulSoup:\n",
    "    with open(htm_path, mode=\"r\") as f:\n",
    "        htm = f.read()\n",
    "        sp = BeautifulSoup(htm, \"html.parser\")\n",
    "        return sp\n",
    "\n",
    "\"\"\"\n",
    "htmファイルには3つのパターンが存在しているため、その都度ハンドリングをする\n",
    "\n",
    "\n",
    "1. smt_textパターン: 属性値にsmt_textを持っているhtm\n",
    "2. textjustify_spanパターン: smt_textを持たず、spanにテキスト情報を記載しているhtm\n",
    "3. text_indent?\n",
    "\"\"\"\n",
    "#-----------------------------------------------------\n",
    "#md_aの章番号を指定\n",
    "#企業によって全角と半角かわかれる可能性があるためどちらも判定\n",
    "# 「１【経営方針、経営環境及び対処すべき課題等】」,「３【経営者による財政状態、経営成績及びキャッシュ・フローの状況の分析】」\n",
    "# after_2017 = ['１', '1','３', '3']\n",
    "\n",
    "def extract_md_a_contents(htm_contents: list, chapter_numbers=['１', '1','３', '3']) -> tuple:\n",
    "    \n",
    "    #MD＆A情報を持つ章を抽出\n",
    "    md_a_contents = []\n",
    "    \n",
    "    for content in htm_contents :\n",
    "        #bs4.element.Tag型ではないとfindメソッドでtypeエラーを起こすため判定\n",
    "        if not isinstance(content, bs4.element.Tag) :\n",
    "            continue\n",
    "        \n",
    "        found = content.find(class_=re.compile(\"smt_head\"))\n",
    "        #findしたけどデータがない(s4.element.Tag型ではない)がない場合.textプロパティを呼び出す際にtypeエラーを起こすため判定\n",
    "        if not  found:\n",
    "            found = content.find(\"h3\")\n",
    "            if not  found:\n",
    "                continue\n",
    "            else: \n",
    "                func = insert_df_by_span_p\n",
    "        else :\n",
    "            func = insert_df_by_smt\n",
    "    \n",
    "        title = found.text\n",
    "        #md_aの章番号を指定\n",
    "            # \"1【経営方針、経営環境及び対処すべき課題等】\",\n",
    "            # \"3【経営者による財政状態、経営成績及びキャッシュ・フローの状況の分析】\"\n",
    "        #企業によって全角と半角か別れる可能性があるためどちらも判定\n",
    "        if title[0] in  chapter_numbers :\n",
    "             md_a_contents.append(content)\n",
    "            \n",
    "    return md_a_contents, func\n",
    "\n",
    "#------------------------------------------\n",
    "def insert_df_by_span_p(md_a_contents: list) -> pd.DataFrame:\n",
    "    \n",
    "    md_a_df = pd.DataFrame()\n",
    "\n",
    "    text_values = []\n",
    "    for content in  md_a_contents : \n",
    "        \n",
    "        # テキスト情報を持つbs4.element.Tag型のみ取得\n",
    "        found_values = content.find_all(\"p\")\n",
    "         # テキストを取得\n",
    "      \n",
    "        for val in found_values:\n",
    "            if val.parent.name in [\"td\" , \"tr\"] or val.parent.parent.name in [\"td\" , \"tr\"]:\n",
    "                continue\n",
    "    \n",
    "            if val.span != None:\n",
    "                text = val.span.text\n",
    "                if len(text) > 1:\n",
    "                    text_values.append(text)\n",
    "            else :\n",
    "                text = val.text\n",
    "                if len(text) > 1:\n",
    "                    text_values.append(text)\n",
    "     \n",
    "        tmp_df = pd.DataFrame()\n",
    "        tmp_df['Text'] = text_values \n",
    "        #pandas.DataFrame型に入れる\n",
    "        md_a_df  = pd.concat([md_a_df ,  tmp_df], axis=0)\n",
    "        \n",
    "    return md_a_df\n",
    "\n",
    "#-----------------------------------------------------\n",
    "def insert_df_by_smt(md_a_contents: list) -> pd.DataFrame:\n",
    "    \n",
    "    md_a_df = pd.DataFrame()\n",
    "\n",
    "    for content in  md_a_contents : \n",
    "        # テキスト情報を持つbs4.element.Tag型のみ取得\n",
    "        found_values = content.find_all(class_=re.compile(\"smt_text\"))\n",
    "         # テキストを取得\n",
    "        text_values = list(filter(lambda val: len(val) > 1, list(map(lambda val: val.text,found_values))))\n",
    "        tmp_df = pd.DataFrame()\n",
    "        tmp_df['Text'] = text_values \n",
    "        #pandas.DataFrame型に入れる\n",
    "        md_a_df  = pd.concat([md_a_df ,  tmp_df], axis=0)\n",
    "        \n",
    "    return md_a_df\n",
    "\n",
    "#----------------------------------------------\n",
    "        \n",
    "def save_md_a_df_to_csv(md_a_df: pd.DataFrame, idx_num: int, date_str: str,  data_frame_name, industry_name) -> None :\n",
    "    #Sampleフォルダの作成\n",
    "    \n",
    "    \n",
    "    filepath = os.getcwd()+ f\"/{data_frame_name}\"\n",
    "    if not os.path.exists(filepath) :\n",
    "        os.mkdir(filepath)\n",
    "        \n",
    "    filepath = os.getcwd()+ f\"/{data_frame_name}/{industry_name}\"\n",
    "    if not os.path.exists(filepath) :\n",
    "        os.mkdir(filepath)\n",
    "    \n",
    "    filepath_before_pandemic  = os.getcwd()+ f\"/{data_frame_name}/{industry_name}\"  +  \"/BeforeSample\"\n",
    "    if not os.path.exists(filepath_before_pandemic) :\n",
    "        os.mkdir(filepath_before_pandemic)\n",
    "    \n",
    "    filepath_transition_period_pandemic = os.getcwd()+ f\"/{data_frame_name}/{industry_name}\"  +  \"/TransitionPeriodSample\"\n",
    "    if not os.path.exists(filepath_transition_period_pandemic) :\n",
    "        os.mkdir(filepath_transition_period_pandemic)\n",
    "    \n",
    "    filepath_after_pandemic = os.getcwd()+ f\"/{data_frame_name}/{industry_name}\"  +  \"/AfterSample\"\n",
    "    if not os.path.exists(filepath_after_pandemic) :\n",
    "        os.mkdir(filepath_after_pandemic)\n",
    "            \n",
    "    \n",
    "    date_dt = dt.strptime(date_str, '%Y-%m-%d')\n",
    "    \n",
    "    before_pandemic_boundary_val = dt(2019, 3, 31)\n",
    "    \n",
    "    transition_period_pandemic_boundary_val = dt(2020, 3, 31)\n",
    "    if  date_dt <= before_pandemic_boundary_val :\n",
    "        md_a_df.to_csv(filepath_before_pandemic+\"/\"+f'{idx_num}_{date_str}.csv')\n",
    "    elif  date_dt <= transition_period_pandemic_boundary_val :\n",
    "        md_a_df.to_csv(filepath_transition_period_pandemic +\"/\"+f'{idx_num}_{date_str}.csv')\n",
    "    else :\n",
    "        md_a_df.to_csv(filepath_after_pandemic+\"/\"+f'{idx_num}_{date_str}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42713a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_frame_name =\"top_20\" \n",
    "data_frame = pd.read_csv(\"/home/jovyan/1CallingEdinetApi\"+f\"/EdinetIdxFiles/{data_frame_name}.csv\", skiprows=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "437ecb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 183\n",
      "2 81\n",
      "3 123\n",
      "4 133\n",
      "5 129\n",
      "6 117\n",
      "7 211\n",
      "8 107\n",
      "9 73\n",
      "10 101\n",
      "11 82\n",
      "12 137\n",
      "13 123\n",
      "14 172\n",
      "15 125\n",
      "16 120\n",
      "17 165\n",
      "18 73\n",
      "19 135\n",
      "20 124\n",
      "21 118\n",
      "22 46\n",
      "23 203\n",
      "24 68\n",
      "25 66\n",
      "26 74\n",
      "27 65\n",
      "28 164\n",
      "29 115\n",
      "30 139\n",
      "31 112\n",
      "32 57\n",
      "33 132\n",
      "34 156\n",
      "35 91\n",
      "36 58\n",
      "37 171\n",
      "38 65\n",
      "39 213\n",
      "40 108\n",
      "41 102\n",
      "42 322\n",
      "43 176\n",
      "44 183\n",
      "45 81\n",
      "46 123\n",
      "47 133\n",
      "48 129\n",
      "49 117\n",
      "50 211\n",
      "51 107\n",
      "52 73\n",
      "53 101\n",
      "54 82\n",
      "55 137\n",
      "56 123\n",
      "57 172\n",
      "58 125\n",
      "59 120\n",
      "60 165\n",
      "61 73\n",
      "62 135\n",
      "63 124\n",
      "64 118\n",
      "65 46\n",
      "66 203\n",
      "67 68\n",
      "68 66\n",
      "69 74\n",
      "70 65\n",
      "71 164\n",
      "72 115\n",
      "73 139\n",
      "74 112\n",
      "75 57\n",
      "76 132\n",
      "77 156\n",
      "78 91\n",
      "79 58\n",
      "80 171\n",
      "81 65\n",
      "82 213\n",
      "83 108\n",
      "84 102\n",
      "85 322\n",
      "86 176\n",
      "87 183\n",
      "88 81\n",
      "89 123\n",
      "90 133\n",
      "91 129\n",
      "92 117\n",
      "93 211\n",
      "94 107\n",
      "95 73\n",
      "96 101\n",
      "97 82\n",
      "98 137\n",
      "99 123\n",
      "100 172\n",
      "101 125\n",
      "102 120\n",
      "103 165\n",
      "104 73\n",
      "105 135\n",
      "106 124\n",
      "107 118\n",
      "108 46\n",
      "109 203\n",
      "110 68\n",
      "111 66\n",
      "112 74\n",
      "113 65\n",
      "114 164\n",
      "115 115\n",
      "116 139\n",
      "117 112\n",
      "118 57\n",
      "119 132\n",
      "120 156\n",
      "121 91\n",
      "122 58\n",
      "123 171\n",
      "124 65\n",
      "125 213\n",
      "126 108\n",
      "127 102\n",
      "128 322\n",
      "129 176\n",
      "130 183\n",
      "131 81\n",
      "132 123\n",
      "133 133\n",
      "134 129\n",
      "135 117\n",
      "136 211\n",
      "137 107\n",
      "138 73\n",
      "139 101\n",
      "140 82\n",
      "141 137\n",
      "142 123\n",
      "143 172\n",
      "144 125\n",
      "145 120\n",
      "146 165\n",
      "147 73\n",
      "148 135\n",
      "149 124\n",
      "150 118\n",
      "151 46\n",
      "152 203\n",
      "153 68\n",
      "154 66\n",
      "155 74\n",
      "156 65\n",
      "157 164\n",
      "158 115\n",
      "159 139\n",
      "160 112\n",
      "161 57\n",
      "162 132\n",
      "163 156\n",
      "164 91\n",
      "165 58\n",
      "166 171\n",
      "167 65\n",
      "168 213\n",
      "169 108\n",
      "170 102\n",
      "171 322\n",
      "172 176\n",
      "173 183\n",
      "174 81\n",
      "175 123\n",
      "176 133\n",
      "177 129\n",
      "178 117\n",
      "179 211\n",
      "180 107\n",
      "181 73\n",
      "182 101\n",
      "183 82\n",
      "184 137\n",
      "185 123\n",
      "186 172\n",
      "187 125\n",
      "188 120\n",
      "189 165\n",
      "190 73\n",
      "191 135\n",
      "192 124\n",
      "193 118\n",
      "194 46\n",
      "195 203\n",
      "196 68\n",
      "197 66\n",
      "198 74\n",
      "199 65\n",
      "200 164\n",
      "201 115\n",
      "202 139\n",
      "203 112\n",
      "204 57\n",
      "205 132\n",
      "206 156\n",
      "207 91\n",
      "208 58\n",
      "209 171\n",
      "210 65\n",
      "211 213\n",
      "212 108\n",
      "213 102\n",
      "214 322\n",
      "215 176\n"
     ]
    }
   ],
   "source": [
    "fetch_md_and_a(data_frame_name, data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b6ce2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

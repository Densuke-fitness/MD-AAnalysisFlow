{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15ccdfc0",
   "metadata": {},
   "source": [
    "\n",
    "# 4.3.3 htmファイルからMD＆A情報としてみなされる文章のみを抽出 \n",
    "\n",
    "　htmファイルから、 BeautifulSoupを用いて MD＆A情報としてみなされる文章のみを抽出した。BeautifulSoupとは、Pythonのライブラリのひとつで、htmlやxmlといったWebページで用いられるデータを構文解析する際に用いられるライブラリである。今回は、 htm形式のテキストファイルからMD&A部分を抜き出し、htm形式のタグの除去等の処理を行って生の文章を抽出した。また文章を抽出においては、加藤・五島(2020)を参考に「１【経営方針、経営環境及び対処すべき課題等】」と「３【経営者による財政状態、経営成績及びキャッシュ・フローの状況の分析】」の 2 つの章のテキストをMD&Aの記述とみなして抽出を行った。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f83123e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "import glob\n",
    "import pandas as pd\n",
    "import bs4\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "\n",
    "def fetch_md_and_a(data_frame_name, data_frame):\n",
    "\n",
    "    htm_files = call_htm_files(data_frame_name)\n",
    "    industry_htm_files_hash = make_industry_htm_files_hash(htm_files , data_frame_name, data_frame)\n",
    "    \n",
    "    sum_csv_output = 1\n",
    "    for industry_name, htm_files in industry_htm_files_hash.items():\n",
    "        \n",
    "        edinet_id_files_hash = {}\n",
    "        #Edninetの識別子ごとでファイル群を管理する\n",
    "        for file in htm_files:\n",
    "            pattern = 'E.*?-'\n",
    "            result = re.search(pattern, file)\n",
    "            start, end = result.span()\n",
    "            identifier = file[start: end-1]\n",
    "\n",
    "            if identifier in edinet_id_files_hash :\n",
    "                edinet_id_files_hash[identifier].append(file)\n",
    "            else:\n",
    "                edinet_id_files_hash[identifier] = [file]\n",
    "        \n",
    "        idx_num = 1\n",
    "        # すでにEdninetの識別子順でソートされている\n",
    "        for identifier, e_files in edinet_id_files_hash.items() :\n",
    "            # 「１【経営方針、経営環境及び対処すべき課題等】」,「３【経営者による財政状態、経営成績及びキャッシュ・フローの状況の分析】」            \n",
    "            #コロナ前, コロナ過渡期, コロナ後の最低３つがなかった時\n",
    "#             if len(e_files) < 3:\n",
    "#                 print(f\"detected: {e_files}\")\n",
    "#                 continue\n",
    "            \n",
    "            date_files_hash = {}\n",
    "            for e_f in e_files:\n",
    "                date_str = get_date_str_val(e_f)\n",
    "                if date_str in date_files_hash :\n",
    "                    date_files_hash[date_str].append(e_f)\n",
    "                else:\n",
    "                    date_files_hash[date_str] = [e_f]\n",
    "            \n",
    "            for date_str, d_files in date_files_hash.items():\n",
    "                \n",
    "                d_files_len = len(d_files)\n",
    "\n",
    "                if d_files_len > 1:\n",
    "                    print(f\"count: {d_files_len}, {d_files}\")\n",
    "                \n",
    "                md_a_df = pd.DataFrame()\n",
    "                idx = 0\n",
    "                while  idx < d_files_len :\n",
    "                    sp = read_file(d_files[idx])\n",
    "                    htm_contents = sp.find('body').div.contents\n",
    "                    md_a_contents, func =  extract_md_a_contents(htm_contents)\n",
    "                    tmp_df = func(md_a_contents)                \n",
    "                    md_a_df  = pd.concat([md_a_df, tmp_df], axis=0)\n",
    "                    idx += 1\n",
    "\n",
    "                save_md_a_df_to_csv(md_a_df, idx_num, date_str , data_frame_name,  industry_name)\n",
    "                #beforeとafterでユニークなindex_numを持つようにする\n",
    "                idx_num += 1\n",
    "                \n",
    "                print(sum_csv_output, len(md_a_df))\n",
    "                sum_csv_output += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18f2d833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_htm_files(data_frame_name):\n",
    "    htm_files = glob.glob(f\"/home/jovyan/work/2UnzippingHtm/UnzipedHtmFiles/{data_frame_name}/**/*.htm\", recursive=True)\n",
    "    return  htm_files\n",
    "\n",
    "\n",
    "def make_industry_htm_files_hash(htm_files , data_frame_name: str, data_frame):\n",
    "    industry_htm_files_hash = {}\n",
    "    \n",
    "    industry_list = make_type_of_industry_list(data_frame)\n",
    "    for industry in industry_list:\n",
    "        industry_htm_files_hash[industry]= list(filter(lambda x:  industry in x , htm_files))\n",
    "    \n",
    "    return  industry_htm_files_hash \n",
    "\n",
    "\n",
    "def make_type_of_industry_list(data_frame : pd.DataFrame, industry_col=\"[業種（東証）]\"):\n",
    "    return [\"サービス業\"]\n",
    "\n",
    "\n",
    "def get_date_str_val(htm_file) :\n",
    "    date_str = htm_file[-20:-10]\n",
    "    return  date_str\n",
    "\n",
    "\n",
    "def read_file(htm_path: str) -> BeautifulSoup:\n",
    "    with open(htm_path, mode=\"r\") as f:\n",
    "        htm = f.read()\n",
    "        sp = BeautifulSoup(htm, \"html.parser\")\n",
    "        return sp\n",
    "\n",
    "\"\"\"\n",
    "htmファイルには3つのパターンが存在しているため、その都度ハンドリングをする\n",
    "\n",
    "\n",
    "1. smt_textパターン: 属性値にsmt_textを持っているhtm\n",
    "2. textjustify_spanパターン: smt_textを持たず、spanにテキスト情報を記載しているhtm\n",
    "3. text_indent?\n",
    "\"\"\"\n",
    "#-----------------------------------------------------\n",
    "#md_aの章番号を指定\n",
    "#企業によって全角と半角かわかれる可能性があるためどちらも判定\n",
    "# 「１【経営方針、経営環境及び対処すべき課題等】」,「３【経営者による財政状態、経営成績及びキャッシュ・フローの状況の分析】」\n",
    "# after_2017 = ['１', '1','３', '3']\n",
    "\n",
    "def extract_md_a_contents(htm_contents: list, chapter_numbers=['１', '1','３', '3']) -> tuple:\n",
    "    \n",
    "    #MD＆A情報を持つ章を抽出\n",
    "    md_a_contents = []\n",
    "    \n",
    "    for content in htm_contents :\n",
    "        #bs4.element.Tag型ではないとfindメソッドでtypeエラーを起こすため判定\n",
    "        if not isinstance(content, bs4.element.Tag) :\n",
    "            continue\n",
    "        \n",
    "        found = content.find(class_=re.compile(\"smt_head\"))\n",
    "        #findしたけどデータがない(s4.element.Tag型ではない)がない場合.textプロパティを呼び出す際にtypeエラーを起こすため判定\n",
    "        if not  found:\n",
    "            found = content.find(\"h3\")\n",
    "            if not  found:\n",
    "                continue\n",
    "            else: \n",
    "                func = insert_df_by_span_p\n",
    "        else :\n",
    "            func = insert_df_by_smt\n",
    "    \n",
    "        title = found.text\n",
    "        #md_aの章番号を指定\n",
    "            # \"1【経営方針、経営環境及び対処すべき課題等】\",\n",
    "            # \"3【経営者による財政状態、経営成績及びキャッシュ・フローの状況の分析】\"\n",
    "        #企業によって全角と半角か別れる可能性があるためどちらも判定\n",
    "        if title[0] in  chapter_numbers :\n",
    "             md_a_contents.append(content)\n",
    "            \n",
    "    return md_a_contents, func\n",
    "\n",
    "#------------------------------------------\n",
    "def insert_df_by_span_p(md_a_contents: list) -> pd.DataFrame:\n",
    "    \n",
    "    md_a_df = pd.DataFrame()\n",
    "\n",
    "    text_values = []\n",
    "    for content in  md_a_contents : \n",
    "        \n",
    "        # テキスト情報を持つbs4.element.Tag型のみ取得\n",
    "        found_values = content.find_all(\"p\")\n",
    "         # テキストを取得\n",
    "      \n",
    "        for val in found_values:\n",
    "            if val.parent.name in [\"td\" , \"tr\"] or val.parent.parent.name in [\"td\" , \"tr\"]:\n",
    "                continue\n",
    "    \n",
    "            if val.span != None:\n",
    "                text = val.span.text\n",
    "                if len(text) > 1:\n",
    "                    text_values.append(text)\n",
    "            else :\n",
    "                text = val.text\n",
    "                if len(text) > 1:\n",
    "                    text_values.append(text)\n",
    "     \n",
    "        tmp_df = pd.DataFrame()\n",
    "        tmp_df['Text'] = text_values \n",
    "        #pandas.DataFrame型に入れる\n",
    "        md_a_df  = pd.concat([md_a_df ,  tmp_df], axis=0)\n",
    "        \n",
    "    return md_a_df\n",
    "\n",
    "#-----------------------------------------------------\n",
    "def insert_df_by_smt(md_a_contents: list) -> pd.DataFrame:\n",
    "    \n",
    "    md_a_df = pd.DataFrame()\n",
    "\n",
    "    for content in  md_a_contents : \n",
    "        # テキスト情報を持つbs4.element.Tag型のみ取得\n",
    "        found_values = content.find_all(class_=re.compile(\"smt_text\"))\n",
    "         # テキストを取得\n",
    "        text_values = list(filter(lambda val: len(val) > 1, list(map(lambda val: val.text,found_values))))\n",
    "        tmp_df = pd.DataFrame()\n",
    "        tmp_df['Text'] = text_values \n",
    "        #pandas.DataFrame型に入れる\n",
    "        md_a_df  = pd.concat([md_a_df ,  tmp_df], axis=0)\n",
    "        \n",
    "    return md_a_df\n",
    "\n",
    "#----------------------------------------------\n",
    "        \n",
    "def save_md_a_df_to_csv(md_a_df: pd.DataFrame, idx_num: int, date_str: str,  data_frame_name, industry_name) -> None :\n",
    "    #Sampleフォルダの作成\n",
    "    \n",
    "    \n",
    "    filepath = os.getcwd()+ f\"/{data_frame_name}\"\n",
    "    if not os.path.exists(filepath) :\n",
    "        os.mkdir(filepath)\n",
    "        \n",
    "    filepath = os.getcwd()+ f\"/{data_frame_name}/{industry_name}\"\n",
    "    if not os.path.exists(filepath) :\n",
    "        os.mkdir(filepath)\n",
    "    \n",
    "    filepath_before_pandemic  = os.getcwd()+ f\"/{data_frame_name}/{industry_name}\"  +  \"/BeforeSample\"\n",
    "    if not os.path.exists(filepath_before_pandemic) :\n",
    "        os.mkdir(filepath_before_pandemic)\n",
    "    \n",
    "    filepath_transition_period_pandemic = os.getcwd()+ f\"/{data_frame_name}/{industry_name}\"  +  \"/TransitionPeriodSample\"\n",
    "    if not os.path.exists(filepath_transition_period_pandemic) :\n",
    "        os.mkdir(filepath_transition_period_pandemic)\n",
    "    \n",
    "    filepath_after_pandemic = os.getcwd()+ f\"/{data_frame_name}/{industry_name}\"  +  \"/AfterSample\"\n",
    "    if not os.path.exists(filepath_after_pandemic) :\n",
    "        os.mkdir(filepath_after_pandemic)\n",
    "            \n",
    "    \n",
    "    date_dt = dt.strptime(date_str, '%Y-%m-%d')\n",
    "    \n",
    "    before_pandemic_boundary_val = dt(2019, 3, 31)\n",
    "    \n",
    "    transition_period_pandemic_boundary_val = dt(2020, 3, 31)\n",
    "    if  date_dt <= before_pandemic_boundary_val :\n",
    "        md_a_df.to_csv(filepath_before_pandemic+\"/\"+f'{idx_num}_{date_str}.csv')\n",
    "    elif  date_dt <= transition_period_pandemic_boundary_val :\n",
    "        md_a_df.to_csv(filepath_transition_period_pandemic +\"/\"+f'{idx_num}_{date_str}.csv')\n",
    "    else :\n",
    "        md_a_df.to_csv(filepath_after_pandemic+\"/\"+f'{idx_num}_{date_str}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42713a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# data_frame_name = \"ave_top_20\"\n",
    "data_frame_name = \"ave_worst_20\"\n",
    "# data_frame_name = \"2021_top_20\"\n",
    "# data_frame_name = \"2021_worst_20\"\n",
    "data_frame = pd.read_csv(\"/home/jovyan/work/1CallingEdinetApi\"+f\"/EdinetIdxFiles/{data_frame_name}.csv\", skiprows=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "437ecb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 142\n",
      "2 138\n",
      "3 150\n",
      "4 47\n",
      "5 93\n",
      "6 79\n",
      "7 84\n",
      "8 74\n",
      "9 72\n",
      "10 62\n",
      "11 61\n",
      "12 64\n",
      "13 120\n",
      "14 127\n",
      "15 89\n",
      "16 178\n",
      "17 185\n",
      "18 84\n",
      "19 123\n",
      "20 102\n",
      "21 115\n",
      "22 39\n",
      "23 59\n",
      "24 49\n",
      "25 42\n",
      "26 52\n",
      "27 88\n",
      "28 32\n",
      "29 27\n",
      "30 26\n",
      "31 181\n",
      "32 154\n",
      "33 127\n",
      "34 97\n",
      "35 138\n",
      "36 107\n",
      "37 133\n",
      "38 155\n",
      "39 144\n",
      "40 129\n",
      "41 136\n",
      "42 151\n",
      "43 96\n",
      "44 119\n",
      "45 109\n",
      "46 131\n",
      "47 144\n",
      "48 160\n",
      "49 165\n",
      "50 171\n",
      "51 169\n",
      "52 164\n",
      "53 148\n",
      "54 114\n",
      "55 96\n",
      "56 94\n",
      "57 99\n",
      "58 57\n",
      "59 89\n",
      "60 118\n",
      "61 40\n",
      "62 47\n",
      "63 61\n",
      "64 164\n",
      "65 174\n",
      "66 162\n",
      "67 63\n",
      "68 75\n",
      "69 53\n",
      "70 115\n",
      "71 120\n",
      "72 128\n",
      "73 111\n",
      "74 56\n",
      "75 48\n",
      "76 133\n",
      "77 133\n",
      "78 155\n",
      "79 58\n",
      "80 50\n",
      "81 52\n",
      "82 65\n",
      "83 70\n",
      "84 89\n",
      "85 96\n",
      "86 95\n",
      "87 98\n",
      "88 42\n",
      "89 44\n",
      "90 82\n",
      "91 185\n",
      "92 177\n",
      "93 164\n",
      "94 57\n",
      "95 49\n",
      "96 69\n",
      "97 123\n",
      "98 114\n",
      "99 116\n",
      "100 60\n",
      "101 60\n",
      "102 73\n",
      "103 153\n",
      "104 172\n",
      "105 174\n",
      "106 52\n",
      "107 64\n",
      "108 84\n",
      "109 106\n",
      "110 170\n",
      "111 173\n"
     ]
    }
   ],
   "source": [
    "fetch_md_and_a(data_frame_name, data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b6ce2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
